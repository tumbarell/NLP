{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport sys\nsys.path.append( \"/kaggle/input/agency-lab-functions\" )\n\npath_essays = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'\npath_models = '/kaggle/working/models/'\npath_files  = '/kaggle/working/files/'\n\ntry:\n    os.mkdir('models')\n    os.mkdir('files') \n    os.mkdir('checkpoints')\nexcept:\n    print('folders already created')\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n#!ls /kaggle/input\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#Installing and choosing java 8. (also worked with java 11)\ntry:\n    os.mkdir('check_java')\n    !cp -r /kaggle/input/java-8/jdk1.8.0_401 /kaggle/working\n    !chmod -R 777 /kaggle/working/jdk1.8.0_401\n    !update-alternatives --install /usr/bin/java java /kaggle/working/jdk1.8.0_401/bin/java 100\n    !update-alternatives --install /usr/bin/javac javac /kaggle/working/jdk1.8.0_401/bin/javac 100\n    !sudo update-alternatives --set java /kaggle/working/jdk1.8.0_401/bin/java\n    os.mkdir('check_java')\nexcept:\n    print(f'Java already installed.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    os.mkdir('check_pyspark')\n    !cp -r /kaggle/input/pyspark/pyspark-3.5.1 /kaggle/working\n    !cp /kaggle/input/spark-nlp/spark_nlp-5.3.3-py2.py3-none-any.whl /kaggle/working\n    !cp /kaggle/input/tumb-py/py4j-0.10.9.7-py2.py3-none-any.whl /kaggle/working\n\n    !pip install --no-index py4j-0.10.9.7-py2.py3-none-any.whl pyspark-3.5.1/. --target=/kaggle/working --find-links=file:///kaggle/working\n    !pip install spark_nlp-5.3.3-py2.py3-none-any.whl --target=/kaggle/working\n\n    !cp /kaggle/input/spark-nlp-assembly/spark-nlp-assembly-5.3.3.jar /kaggle/working/pyspark/jars\n\n    !chmod -R 777 /kaggle/working/pyspark/./bin/\n    os.mkdir('check_pyspark')\nexcept:\n    print('pyspark already installed')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport shutil\nimport pyspark\nimport subprocess\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nimport sparknlp\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import Tokenizer as Tknzr, Bucketizer, StopWordsRemover\nfrom pyspark.ml.linalg import VectorUDT, Vectors\nfrom pyspark.ml.functions import vector_to_array, array_to_vector\nfrom pyspark.ml.feature import VectorAssembler, MaxAbsScaler, MinMaxScaler\nfrom pyspark.ml.regression import LinearRegression, LinearRegressionModel, RandomForestRegressor,\\\n                                    RandomForestRegressionModel, GBTRegressor, GBTRegressionModel\nfrom pyspark.ml.classification import LogisticRegression, OneVsRest, OneVsRestModel,\\\n                                        LogisticRegressionModel,\\\n                                        RandomForestClassifier, RandomForestClassificationModel\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\nimport lalab_functions as lab\nimport similarity_measures_2 as sims \nfrom IPython.core.display import HTML\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\nfrom xgboost.spark import SparkXGBClassifier, SparkXGBClassifierModel\nfrom functools import reduce\n\nfrom pyspark.ml.clustering import KMeans, KMeansModel\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nconf = SparkConf() \\\n    .setAppName(\"YourAppName\") \\\n    .setMaster(\"local[*]\") \\\n    .set(\"spark.driver.memory\", \"10g\") \\\n    .set(\"spark.executor.memory\", \"15g\") \\\n    .set(\"spark.executor.cores\", \"8\") \\\n    .set(\"spark.executor.instances\", \"1\") \\\n    .set(\"spark.default.parallelism\", \"8\") \\\n    .set(\"spark.sql.shuffle.partitions\", \"4\") \\\n    .set(\"spark.memory.fraction\", \"0.8\") \\\n    .set(\"spark.memory.storageFraction\", \"0.40\")\\\n    .set(\"spark.jars\", \n         \"/kaggle/working/pyspark/jars/spark-nlp-assembly-5.3.3.jar\"\n        )\\\n    .set(\"spark.executorEnv.JAVA_HOME\", \"/kaggle/working/jdk1.8.0_401\")\\\n    .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n    .set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n\n#     .set(\"spark.emptyArrowBatchCheck\", True)\n\n#Enabling spark.databricks.pyspark.emptyArrowBatchCheck prevents \n#a NoSuchElementException error from occurring when the Arrow batch size is 0.\n#\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\"\n   \nspark = SparkSession.builder \\\n    .config(conf=conf) \\\n    .getOrCreate()\n\nsc = spark.sparkContext\n# Get the number of cores\nnum_cores = sc.defaultParallelism\n# Get the number of executors\nnum_executors = sc._jsc.sc().getExecutorMemoryStatus().keySet().size()\nprint(\"Number of cores:\", num_cores)\nprint(\"Number of executors:\", num_executors)\n# Get the SparkConf object\nconf = spark.sparkContext.getConf()\n# Get the memory configurations for driver and executors\ndriver_memory = conf.get(\"spark.driver.memory\", \"Not set\")\nexecutor_memory = conf.get(\"spark.executor.memory\", \"Not set\")\nprint(\"Driver memory:\", driver_memory)\nprint(\"Executor memory:\", executor_memory)\n# Set the checkpoint directory\nspark.sparkContext.setCheckpointDir('checkpoints')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pipeline to process documents\ndocument = DocumentAssembler() \\\n    .setInputCol(\"full_text\") \\\n    .setOutputCol(\"document\")\nsentenceDetector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\")\nuse = UniversalSentenceEncoder.load('/kaggle/input/sentence-encoder')\\\n                 .setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")\ntokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\\\n                        .setSplitPattern(\"\\\\b\\\\w+\\\\b\")  # Split by word boundaries\nnormalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized_tokens\") \\\n                .setLowercase(True).setCleanupPatterns([\"[^\\\\w\\\\s]\"])# Retain numbers as tokens  \nstopWords = StopWordsCleaner().setInputCols([\"normalized_tokens\"]) \\\n               .setOutputCol(\"cleanTokens\").setCaseSensitive(False)\nlemmatizer = LemmatizerModel.load('/kaggle/input/spacylemmatizer')\\\n                .setInputCols([\"cleanTokens\"]).setOutputCol(\"lemmas\")\npostagger = PerceptronModel.load(\"/kaggle/input/pos-tagger\")\\\n             .setInputCols(\"document\", \"lemmas\").setOutputCol(\"pos\")\nuse_pos_pipeline = Pipeline().setStages([\n      document,\n      sentenceDetector,\n      use,\n      tokenizer,\n      normalizer,\n      stopWords,\n      lemmatizer,\n      postagger\n    ])\n\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"full_text\")\nusePosModel = use_pos_pipeline.fit(empty_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading models to process word embeddings\nbinary_models = {}\nscores = [1,2,3,4,5]\nbinary_path = '/kaggle/input/use-xgb-5binaries-imp-feats-600-estimators/'\\\n        +'use_xgb_super_binaries_5score_improved_features_600_estimators/'\nbinary_folders = lab.list_folders_in_folder(binary_path)\nmodel_class = SparkXGBClassifierModel\nfor score in scores:\n    label = 'xgb_target_'+str(score)\n    key_folder = [mname for mname in binary_folders \n              if label in mname][0]\n    \n    _path = binary_path+key_folder+'/'\n    model_word = label\n    binary_models[score] = lab.load_models_in_folder(_path, model_class,\n                                              model_word)[0] \n    \n\nseparators_keys = [\n    (1,2), (1,3), (1,4), (1,5),\n    (2,3), (2,4), (2,5),\n    (3,4), (3,5), \n    (4,5) \n]\nsep_models = {}\n\nsep_path = '/kaggle/input/use-xgb-psep-5scores-'\\\n            +'impfeat-300-estimators/use_xgb_pure_sep_5scores_improved_features_300_estimators/'\n# sep_path = '/kaggle/input/use-xgb-psep-imp-feats-1000-est-30-depth/'\\\n#                 +'use_xgb_pure_sep_5scores_improved_features_1000_estimators_30_depth/'\n# sep_path = '/kaggle/input/use-xgb-psep-optimized-imp-feats/use_xgb_psep_optimized_by_accuracy/'\nsep_folders = lab.list_folders_in_folder(sep_path)\nprint(sep_folders)\nfor key in separators_keys:\n\n    label = f'psep_for_'+'_'.join(str(p) for p in key)\n    \n    key_folder = [mname for mname in sep_folders \n              if label in mname][0]\n    _path = sep_path+key_folder+'/'\n    model_word = 'psep_'+'_'.join(str(p) for p in key)\n    sep_models[key] = lab.load_models_in_folder(_path, model_class,\n                                              model_word)[0]\n    \n#     model_word = 'xgb_'+'_'.join(str(p) for p in key)\n#     sep_models[key] = lab.load_models_in_folder(sep_path, model_class,\n#                                               model_word)[0]\n    print(f'key: {key} model: {sep_models[key]}')\n    \nqtern_models = {}    \nqtern_path = '/kaggle/input/use-xgb-qterns-1000-estimators-30-depth/'\\\n        +'use_xgb_super_qtsep_5scores_improved_feats_1000_estimators+30_depth/'\nqtern_folders = lab.list_folders_in_folder(qtern_path)\nprint(qtern_folders)\nfor key in separators_keys:\n\n    label = f'qsep_for_'+''.join(str(p) for p in key)\n    key_folder = [mname for mname in qtern_folders \n              if label in mname][0]\n    _path = qtern_path+key_folder+'/'\n    model_word = 'qtsep_0-'+''.join(str(p) for p in key)\n    qtern_models[key] = lab.load_models_in_folder(_path, model_class,\n                                              model_word)[0]\n    print(f'key: {key} model: {qtern_models[key]}')\n    \nsep_56 = model_class.load('/kaggle/input/use-xgb-psep-56-'\\\n                +'imp-feats-acc075/use_xgb_sep_56_imp_feats_acc075')\nuse_xgb_general = model_class.load('/kaggle/input/use-xgb-general-'\\\n                                   +'from-combined-models/xgb_general_from_combined_models')\n\n    \nprint('models loaded')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint('Evaluating the essays')\nt0 = time.time()\nessays = spark.createDataFrame(pd.read_csv(path_essays+'test.csv'))\\\n            .withColumn('batches', (10*F.rand()).cast(IntegerType()))\\\n            .cache()\n# essays = df_test\\\n#                 .withColumn('score_5', F.when(F.col('score')==6,5)\\\n#                           .otherwise(F.col('score')))\\\n#                 .withColumnRenamed('score','original_score')\\\n#             .withColumn('batches', (10*F.rand()).cast(IntegerType()))\n\nbatches = essays.select('batches').distinct().toPandas()['batches'].tolist()\nprint(f'batches: {len(batches)}')\ncounter = 0\n\nfor batch in batches:\n    \n    tm = time.time()\n    \n    counter += 1\n    \n    df2evaluate = essays.filter(F.col('batches')==batch)\n    \n    df2evaluate = usePosModel.transform(df2evaluate)\\\n                   .withColumn('p_features', array_to_vector(F.col('sentence_embeddings').embeddings[0]))\n\n    df2evaluate = lab.sentences_and_embeddings(df2evaluate)\n    df2evaluate = lab.features_for_modeling(df2evaluate)\n    columns_for_assembling = [ 'p_features', 'words_fraction',  \n                'nouns_fraction', 'verbs_fraction', 'others_fraction', \n               'uniqwd_fraction', 'toksxsent_fraction','raw_length_fraction']\n    df2evaluate = lab.vector_assembler(df2evaluate, columns_for_assembling, 'features')\n    \n    cols2keep = ['essay_id','features']\n    for col in df2evaluate.columns:\n        if col not in cols2keep:\n            df2evaluate = df2evaluate.drop(col)\n    print(f'   Deleting unnecessary columns. Entries: {df2evaluate.count()}')\n    bincols = []\n    for score in scores:\n        model = binary_models[score]\n        colname = 'pred_'+str(score)\n        df2evaluate = model.transform(df2evaluate)\\\n                .withColumn(colname,F.col(model.getPredictionCol()).cast(DoubleType()))\n        bincols.append(colname)\n        cols2keep.append(colname)\n        df2evaluate = df2evaluate.select(cols2keep)\n    sepcols = []\n    for key in separators_keys:\n        model = sep_models[key]\n        colname = 'pred_'+'_'.join(str(p) for p in key)\n        df2evaluate = model.transform(df2evaluate)\\\n                .withColumn(colname,F.col(model.getPredictionCol()).cast(DoubleType()))\\\n                .withColumn(colname,F.when(F.col(colname)==1,key[1]).otherwise(key[0]).cast(DoubleType()))\n        sepcols.append(colname)\n        cols2keep.append(colname)\n        df2evaluate = df2evaluate.select(cols2keep)\n    \n    df2evaluate = df2evaluate.checkpoint()\n    \n#     df2evaluate = df2evaluate.withColumn('weigthed_prediction', \n#                             lab.weigthed_predictions()(F.array(*sepcols))[0].cast(DoubleType()))\n#     cols2keep.append('weigthed_prediction')\n    for key in separators_keys:\n        colname = 'pred_'+'_'.join(str(p) for p in key)\n        df2evaluate = df2evaluate\\\n                    .withColumn(colname,F.when(F.col(colname)==key[1],1).otherwise(0).cast(DoubleType()))\n\n    qterncols = []\n    for key in separators_keys:\n        model = qtern_models[key]\n        colname = 'pred_0-'+''.join(str(p) for p in key)\n        df2evaluate = model.transform(df2evaluate)\\\n                .withColumn(colname,F.col(model.getPredictionCol()).cast(DoubleType()))\n        qterncols.append(colname)\n        cols2keep.append(colname)\n        df2evaluate = df2evaluate.select(cols2keep) \n\n    df2evaluate = df2evaluate.withColumn('features_2', array_to_vector(F.array(*(bincols+sepcols+qterncols))))\n    \n    df2evaluate = use_xgb_general.transform(df2evaluate)\\\n                    .withColumn('xgb_score', (F.col('xgb_score')+1).cast(IntegerType()))\\\n                    .withColumnRenamed('xgb_score', 'score')\\\n                    .select('essay_id', 'score')\n        \n#     df2evaluate = df2evaluate.withColumn('score', \n#                             lab.weigthed_predictions()(F.array(*sepcols))[0].cast(IntegerType()))\\\n#                             .select('essay_id', 'score')\n    \n#     df_56 = df2evaluate.filter(F.col('score')==5)\n    \n#     df2evaluate = df2evaluate.filter(F.col('score')!=5).select('essay_id', 'score')\n    \n#     if df_56.count() > 0:\n#         df_56 = df_56.select(initial_columns)\n#         df_56 = sep_56.transform(df_56)\\\n#                     .withColumn('score', F.col(sep_56.getPredictionCol()).cast(IntegerType()))\\\n#                     .withColumn('score', F.when(F.col('score')==1,6).otherwise(5))\\\n#                     .select('essay_id', 'score')\n#         df2evaluate = df2evaluate.union(df_56)\n                    \n    if counter == 1:   \n        df2evaluate.write.mode('overwrite').parquet(path_files+'submission.parquet')\n    else:\n        df2evaluate.write.mode('append').parquet(path_files+'submission.parquet')\n    \n    print(f'Evaluation of batch {counter}/{len(batches)} in '\\\n          +f'{round((time.time()-tm)/60,2)} minutes. '\\\n          +f'Entries: {df2evaluate.count()}. '\\\n          +f'Time elapsed: {round((time.time()-t0)/60,2)} minutes.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nevaluated_essays = spark.read.parquet(path_files+'submission.parquet')\nevaluated_essays.toPandas().to_csv('submission.csv', index=False)\n\nfolders = lab.list_folders_in_folder('/kaggle/working/')\nfiles = lab.list_files_in_folder('/kaggle/working/')\n\nfor fold in folders:\n    shutil.rmtree(fold)\n\nfor file in files:\n    if file != 'submission.csv':\n        os.remove(file)","metadata":{},"execution_count":null,"outputs":[]}]}